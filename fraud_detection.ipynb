{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import *\n",
    "from time import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.python.keras import optimizers, regularizers\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "            getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(28,), activity_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        model.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        model.add(Dense(2))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        # Set and Serialize Optimizer\n",
    "        optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "        opt_conf = optimizers.serialize(optimizer_conf)\n",
    "        \n",
    "        # Initialize SparkML Estimator and Get Settings\n",
    "        estimator = ElephasEstimator()\n",
    "        estimator.setFeaturesCol(\"features\")\n",
    "        estimator.setLabelCol(\"Class\")\n",
    "        estimator.set_keras_model_config(model.to_yaml())\n",
    "        estimator.set_categorical_labels(True)\n",
    "        estimator.set_nb_classes(2)\n",
    "        estimator.set_num_workers(1)\n",
    "        estimator.set_epochs(25) \n",
    "        estimator.set_batch_size(64)\n",
    "        estimator.set_verbosity(1)\n",
    "        estimator.set_validation_split(0.10)\n",
    "        estimator.set_optimizer_config(opt_conf)\n",
    "        estimator.set_mode(\"synchronous\")\n",
    "        estimator.set_loss(\"binary_crossentropy\")\n",
    "        estimator.set_metrics(['acc'])\n",
    "\n",
    "        # Create Deep Learning Pipeline\n",
    "        self.dl_pipeline = Pipeline(stages=[estimator])\n",
    "        \n",
    "    def train(self, data):\n",
    "        dl_pipeline = self.dl_pipeline\n",
    "        start = time()\n",
    "        self.model = dl_pipeline.fit(data)\n",
    "        print('Elapsed time is: {}'.format(time()-start))\n",
    "    \n",
    "    def pred(self, data):\n",
    "        return self.model.transform(data).select('Class', \"prediction\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions: 8\n",
      "Created df with: 284807, 31\n",
      "Elapsed time is: 18.282267570495605\n"
     ]
    }
   ],
   "source": [
    "# def main(ram, duplicate, splitation=[0.7, 0.1, 0.2]):\n",
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "start = time()\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/creditcard.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)\n",
    "preprocessor = Preprocess(detector.rep_data)\n",
    "preprocessor.robust_scale(['Time', 'Amount'])\n",
    "preprocessor.outlier_removal(['V14', 'V12', 'V10'], rpt=False)\n",
    "detector.data = preprocessor.assemble_features()\n",
    "print('Elapsed time is: {}'.format(time()-start))\n",
    "train, validation, test = detector.data.randomSplit(splitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n",
      "Elapsed time is: 97.74211144447327\n"
     ]
    }
   ],
   "source": [
    "model = Predictor()\n",
    "model.train(train)\n",
    "result_train = model.pred(train)\n",
    "result_test = model.pred(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = result_train.withColumn(\"class\", result_train[\"Class\"].cast(DoubleType()))\n",
    "result_test = result_test.withColumn(\"class\", result_test[\"Class\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199301\n",
      "56858\n"
     ]
    }
   ],
   "source": [
    "print(result_train.count())\n",
    "print(result_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9977320735972223\n",
      "accuracy: 0.9975201378873685\n",
      "recall: 0.9983529671698151\n",
      "recall: 0.9982047310616727\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "evaluator.accuracy(result_train)\n",
    "evaluator.accuracy(result_test)\n",
    "evaluator.recall(result_train)\n",
    "evaluator.recall(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
