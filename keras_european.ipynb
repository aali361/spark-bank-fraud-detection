{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import *\n",
    "from time import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, LSTM, GRU, Input, SimpleRNN\n",
    "from tensorflow.python.keras import optimizers, regularizers\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").getOrCreate()\n",
    "#             config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "#             config(\"spark.task.cpus\", \"6\".format(ram)).\\\n",
    "#             getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "\n",
    "        \n",
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self, label=\"class\", prediction=\"prediction\"):\n",
    "        self.label = label\n",
    "        self.prediction = prediction\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=self.label, predictionCol=self.prediction, metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=self.label, predictionCol=self.prediction, metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "    \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=self.label, predictionCol=self.prediction, metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data))) \n",
    "    \n",
    "    def auc_roc(self, data):\n",
    "        roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=self.prediction, labelCol=self.label, metricName=\"areaUnderROC\")\n",
    "        auc_roc = roc_evaluator.evaluate(data)\n",
    "        print(f'auc_roc: {auc_roc}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keras_Predictor():\n",
    "    def __init__(self):\n",
    "        self.model = None \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_shape=(28,), activity_regularizer=regularizers.l2(0.01)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dropout(rate=0.3))\n",
    "        self.model.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dropout(rate=0.3))\n",
    "        self.model.add(Dense(2))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        # Set and Serialize Optimizer\n",
    "        optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "        opt_conf = optimizers.serialize(optimizer_conf)\n",
    "        \n",
    "        # Initialize SparkML Estimator and Get Settings\n",
    "        estimator = ElephasEstimator()\n",
    "        estimator.setFeaturesCol(\"features\")\n",
    "        estimator.setLabelCol(\"Class\")\n",
    "        estimator.set_keras_model_config(self.model.to_yaml())\n",
    "        estimator.set_categorical_labels(True)\n",
    "        estimator.set_nb_classes(2)\n",
    "        estimator.set_num_workers(1)\n",
    "        estimator.set_epochs(25) \n",
    "        estimator.set_batch_size(64)\n",
    "        estimator.set_verbosity(1)\n",
    "        estimator.set_validation_split(0.10)\n",
    "        estimator.set_optimizer_config(opt_conf)\n",
    "        estimator.set_mode(\"synchronous\")\n",
    "        estimator.set_loss(\"binary_crossentropy\")\n",
    "        estimator.set_metrics(['acc'])\n",
    "\n",
    "        # Create Deep Learning Pipeline\n",
    "        self.dl_pipeline = Pipeline(stages=[estimator])\n",
    "        \n",
    "    def train(self, data):\n",
    "        dl_pipeline = self.dl_pipeline\n",
    "        start = time()\n",
    "        self.model = dl_pipeline.fit(data)\n",
    "        print('Elapsed time is: {}'.format(time()-start))\n",
    "    \n",
    "    def pred(self, data):\n",
    "        return self.model.transform(data).select('Class', \"prediction\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/creditcard.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)\n",
    "start = time()\n",
    "preprocessor = Preprocess(detector.rep_data)\n",
    "preprocessor.robust_scale(['Time', 'Amount'])\n",
    "preprocessor.outlier_removal(['V14', 'V12', 'V10'], rpt=False)\n",
    "detector.data = preprocessor.assemble_features()\n",
    "train, validation, test = detector.data.randomSplit(splitation)\n",
    "print('Elapsed time is: {}'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", numTrees=10)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Class\", \"features\").show(5)\n",
    "predictions = predictions.withColumn(\"Class\", predictions[\"Class\"].cast(DoubleType()))\n",
    "\n",
    "roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"Class\", labelCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "print('ROC: {}'.format(roc_evaluator.evaluate(predictions)))\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"Class\", metricName=\"accuracy\")\n",
    "print('accuracy: {}'.format(accuracy_evaluator.evaluate(predictions)))\n",
    "print('Elapsed time is: {}'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = Keras_Predictor()\n",
    "k.train(train)\n",
    "predictions = k.pred(test)\n",
    "\n",
    "e = Evaluator(label=\"Class\", prediction=\"prediction\")\n",
    "e.accuracy(predictions)\n",
    "e.auc_roc(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
