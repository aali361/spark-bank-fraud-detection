{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from scipy import stats\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "            getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "\n",
    "        \n",
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "    \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions: 8\n",
      "Created df with: 594643, 10\n"
     ]
    }
   ],
   "source": [
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/bank_sim.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def drop_columns(self, columns = ['zipcodeOri', 'zipMerchant']):\n",
    "        for col in columns:\n",
    "            self.df = self.df.drop('zipcodeOri')\n",
    "            self.df = self.df.drop('zipMerchant')\n",
    "    \n",
    "    def index_column(self, df, column):\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"Index\")\n",
    "        df = indexer.fit(df).transform(df)\n",
    "        df = df.withColumn(column, df[column+\"Index\"].cast(IntegerType()))\n",
    "        df = df.drop(column+\"Index\")\n",
    "        return df\n",
    "    \n",
    "    def columns_indexing(self, columns = ['merchant', 'category', 'customer', 'age', 'gender']):\n",
    "        for column in columns:\n",
    "            self.df = self.index_column(self.df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(detector.rep_data)\n",
    "preprocess.drop_columns()\n",
    "preprocess.columns_indexing()\n",
    "detector.rep_data = preprocess.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.merchant_fraud_probablites = None\n",
    "    \n",
    "    def merchant_fraud_probablity(self, merchant):\n",
    "        merchant_df = self.df.filter(self.df.merchant==merchant)\n",
    "        return merchant_df.filter(merchant_df.fraud==1).count()/merchant_df.count()\n",
    "    \n",
    "    def merchants_fruad_probablity(self):\n",
    "        merchants = self.df.toPandas()['merchant'].unique()\n",
    "        self.merchant_fraud_probablites = {}\n",
    "        for merchant in merchants:\n",
    "            self.merchant_fraud_probablites[merchant] = self.merchant_fraud_probablity(int(merchant))\n",
    "        return self.merchant_fraud_probablites\n",
    "    \n",
    "    @staticmethod\n",
    "    def merchant_probablity(merchant, x):\n",
    "        return probs[merchant]\n",
    "    \n",
    "    def extract_merchant_fraud_probablity(self, probs):\n",
    "        self.df = self.df.rdd.map(lambda x: x + (func1(x[\"merchant\"], probs),)).toDF(self.df.columns + [\"merchantProbablity\"])\n",
    "        \n",
    "    def customer_trans(self):\n",
    "        _customers_trans = {}\n",
    "        for row in self.df.collect():\n",
    "            customer_trans = _customers_trans.get(row.customer, None)\n",
    "            if customer_trans is None:\n",
    "                customer_trans = {}\n",
    "            if row.step in customer_trans:\n",
    "                customer_trans[row.step+1] = row.amount\n",
    "            else:\n",
    "                customer_trans[row.step] = row.amount\n",
    "            _customers_trans[row.customer] = customer_trans\n",
    "        customer_trans_broadcast = detector.spark.sparkContext.broadcast(_customers_trans)\n",
    "        return customer_trans_broadcast\n",
    "        \n",
    "    def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "        customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "        trans = [v for k,v in customer_trans.items() if k < step]\n",
    "    #     check this, must be 0 or -1\n",
    "        prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "        if int(prev_amount) == 0: return 0.0\n",
    "        variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "        return variance\n",
    "        \n",
    "    def extract_last_variance(self, customer_trans_broadcast):\n",
    "        self.df = self.df.rdd.map(lambda x: x + (last_variance(int(x[\"customer\"]), int(x['step']), int(x[\"amount\"]), customer_trans_broadcast),)).toDF(self.df.columns + [\"lastVariance\"])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(merchant, probs):\n",
    "    return probs[merchant]\n",
    "\n",
    "def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "    customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "    trans = [v for k,v in customer_trans.items() if k < step]\n",
    "    prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "    if int(prev_amount) == 0: return 0.0\n",
    "    variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "    return variance\n",
    "\n",
    "featureExtraction = FeatureExtraction(detector.rep_data)\n",
    "featureExtraction.extract_merchant_fraud_probablity(featureExtraction.merchants_fruad_probablity())\n",
    "customer_trans_broadcast = featureExtraction.customer_trans()\n",
    "df = featureExtraction.extract_last_variance(customer_trans_broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "df = df.withColumn('lastVariance', func.round(df[\"lastVariance\"], 4))\n",
    "df = df.withColumn('merchantProbablity', func.round(df[\"merchantProbablity\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('features')\n",
    "# inputCols = [\n",
    "#  'customer',\n",
    "#  'age',\n",
    "#  'gender',\n",
    "#  'merchant',\n",
    "#  'category',\n",
    "#  'amount',\n",
    "    \n",
    "#  'merchantProbablity',\n",
    "#  'lastVariance'\n",
    "# ]\n",
    "# assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "# df = assembler.transform(df)\n",
    "\n",
    "# train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "\n",
    "def vectorizerFunction(dataInput, TargetFieldName):\n",
    "    if(dataInput.select(TargetFieldName).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    columnNames = list(dataInput.columns)\n",
    "    columnNames.remove(TargetFieldName)\n",
    "    dataInput = dataInput.select((','.join(columnNames)+','+TargetFieldName).split(',')) \n",
    "    assembler=VectorAssembler(inputCols = columnNames, outputCol = 'features')\n",
    "    pos_vectorized = assembler.transform(dataInput)\n",
    "    vectorized = pos_vectorized.select('features',TargetFieldName).withColumn('label',pos_vectorized[TargetFieldName]).drop(TargetFieldName)\n",
    "    return vectorized\n",
    "\n",
    "def SmoteSampling(vectorized, k = 5, minorityClass = 1, majorityClass = 0, percentageOver = 200, percentageUnder = 100):\n",
    "    if(percentageUnder > 100|percentageUnder < 10):\n",
    "        raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "    if(percentageOver < 100):\n",
    "        raise ValueError(\"Percentage Over must be in at least 100\");\n",
    "    dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "    dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "    feature = dataInput_min.select('features').rdd.map(lambda x: x[0]).collect()\n",
    "    feature = np.asarray(feature)\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=k, algorithm='auto').fit(feature)\n",
    "    neighbours =  nbrs.kneighbors(feature)\n",
    "    gap = neighbours[0]\n",
    "    neighbours = neighbours[1]\n",
    "    pos_ListArray = dataInput_min.drop('label').rdd.map(lambda x : list(x)).collect()\n",
    "    min_Array = list(pos_ListArray)\n",
    "    newRows = []\n",
    "    nt = len(min_Array)\n",
    "    nexs = percentageOver/100\n",
    "    for i in range(nt):\n",
    "        for j in range(int(nexs)):\n",
    "            neigh = random.randint(1,k)\n",
    "            difs = min_Array[neigh][0] - min_Array[i][0]\n",
    "            newRec = (min_Array[i][0]+random.random()*difs)\n",
    "            newRows.insert(0,(newRec))\n",
    "    newData_rdd = detector.spark.sparkContext.parallelize(newRows)\n",
    "    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n",
    "    new_data = newData_rdd_new.toDF()\n",
    "    new_data_minor = dataInput_min.unionAll(new_data)\n",
    "    new_data_major = dataInput_maj.sample(False, (float(percentageUnder)/float(100)))\n",
    "    return new_data_major.unionAll(new_data_minor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(train, test, sampling=True):\n",
    "    train = vectorizerFunction(train, 'fraud')\n",
    "    test = vectorizerFunction(test, 'fraud')\n",
    "\n",
    "    # percentage under reduce major class. 100 mean all instances of major class\n",
    "    # percentage over increase minor class. 200 mean each sample of minor class must be doubled\n",
    "    if sampling:\n",
    "        train = SmoteSampling(train, k=2, minorityClass=1, majorityClass=0, percentageOver=200, percentageUnder=100)\n",
    "\n",
    "    train = train.withColumnRenamed('label','fraud')\n",
    "    test = test.withColumnRenamed('label','fraud')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count: 417107, validation count: 59473, test count: 118063\n"
     ]
    }
   ],
   "source": [
    "# must remove\n",
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "# train, test = smote(train, test, False)\n",
    "print('train count: {}, validation count: {}, test count: {}'.format(train.count(), validation.count(), test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panda = df.toPandas()\n",
    "y = np.array(df_panda['fraud'], dtype='float')\n",
    "X = df_panda.drop(['fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(default=5):\n",
    "    auc_roc_arr_method1 = []\n",
    "    auc_roc_arr_method2 = []\n",
    "    for i in range(default):\n",
    "        skf = StratifiedKFold(n_splits=2)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            X_train.insert(1, 'fraud', y_train)\n",
    "            X_test.insert(1, 'fraud', y_test)\n",
    "            df_train = detector.spark.createDataFrame(X_train)\n",
    "            df_test = detector.spark.createDataFrame(X_test)\n",
    "            df_train, df_test = smote(df_train, df_test, False)\n",
    "            auc_roc_arr_method1.append(simple_ffnn(df_train, df_test))\n",
    "            auc_roc_arr_method2.append(ensemble(df_train, df_test))\n",
    "    return auc_roc_arr_method1, auc_roc_arr_method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_arr_method1, auc_roc_arr_method2 = repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pvalue(num_round, result_method1, result_method2):\n",
    "    num_round = 10\n",
    "    score_diff = []\n",
    "    for i in range(num_round):\n",
    "        score_diff.append(auc_roc_arr_method1[i]-auc_roc_arr_method2[i])\n",
    "    score_diff\n",
    "    avg_diff = np.mean(score_diff)\n",
    "\n",
    "    numerator = avg_diff * np.sqrt(num_round)\n",
    "    denominator = np.sqrt(sum([(diff - avg_diff)**2 for diff in score_diff])\n",
    "                          / (num_round - 1))\n",
    "    t_stat = numerator / denominator\n",
    "    pvalue = stats.t.sf(np.abs(t_stat), num_round - 1)*2.\n",
    "\n",
    "    # interpret the result\n",
    "    if pvalue <= 0.05:\n",
    "        print('Difference between mean performance is probably real')\n",
    "    else:\n",
    "        print('Algorithms probably have the same performance')\n",
    "    return pvalue, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pvalue(10, auc_roc_arr_method1, auc_roc_arr_method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply using FFNN\n",
    "def simple_ffnn(train, test):\n",
    "    print('simple FFNN')\n",
    "    layers = [9, 5, 2]\n",
    "    start = time()\n",
    "    FNN = MultilayerPerceptronClassifier(labelCol=\"fraud\", featuresCol=\"features\", maxIter=100, layers=layers, blockSize=128, seed=123)\n",
    "    pipeline = Pipeline(stages=[FNN])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    print(time()-start)\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "#     predictions.show(5)\n",
    "    return auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to dense vector\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).toDF(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(train, test):\n",
    "    print('Ensemble')\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(labelCol=\"fraud\", featuresCol=\"features\", numTrees=10, seed=123, maxDepth=8, maxBins=10)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model_rf = pipeline.fit(train)\n",
    "    predictions_rf = model_rf.transform(train)\n",
    "    predictions_train_rf = predictions_rf.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_rf = model_rf.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[dt])\n",
    "    model_dt = pipeline.fit(train)\n",
    "    predictions_dt = model_dt.transform(train)\n",
    "    predictions_train_dt = predictions_dt.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_dt = model_dt.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(labelCol=\"fraud\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    model_lr = pipeline.fit(train)\n",
    "    predictions_lr = model_lr.transform(train)\n",
    "    predictions_train_lr = predictions_lr.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_lr = model_lr.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # Gradient Boosted\n",
    "    gb = GBTClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[gb])\n",
    "    model_gb = pipeline.fit(train)\n",
    "    predictions_gb = model_gb.transform(train)\n",
    "    predictions_train_gb = predictions_gb.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_gb = model_gb.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # SVM\n",
    "    svm = LinearSVC(labelCol=\"fraud\", maxIter=10, regParam=0.1)\n",
    "    pipeline = Pipeline(stages=[svm])\n",
    "    model_svm = pipeline.fit(train)\n",
    "    predictions_svm = model_svm.transform(train)\n",
    "    predictions_train_svm = predictions_svm.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_svm = model_svm.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "    \n",
    "    train_labels = train.select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "    test_labels = test.select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "    ffnn = []\n",
    "    for i in range(train.count()):\n",
    "        ffnn.append([predictions_train_rf[i], predictions_train_dt[i], predictions_train_lr[i], predictions_train_gb[i], predictions_train_svm[i], train_labels[i]])\n",
    "    models_predict = []\n",
    "    for i in range(test.count()):\n",
    "        models_predict.append([predictions_test_rf[i], predictions_test_dt[i], predictions_test_lr[i], predictions_test_gb[i], predictions_test_svm[i], test_labels[i]])\n",
    "        \n",
    "    train_df = detector.spark.createDataFrame(ffnn)\n",
    "    train_df = transData(train_df)\n",
    "    print(train_df.count())\n",
    "#     train_df.show()\n",
    "    \n",
    "    test_df = detector.spark.createDataFrame(models_predict)\n",
    "    test_df = transData(test_df)\n",
    "    print(test_df.count())\n",
    "#     test_df.show()\n",
    "    \n",
    "    layers = [5, 5, 4, 4, 3, 2]\n",
    "    # layers = [5, 2]\n",
    "    \n",
    "    FNN = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=100, layers=layers, blockSize=128, seed=123)\n",
    "    pipeline = Pipeline(stages=[FNN])\n",
    "    model = pipeline.fit(train_df)\n",
    "    predictions = model.transform(test_df)\n",
    "#     predictions.show(5)\n",
    "    \n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "    return auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RF(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol=\"fraud\", featuresCol=\"features\", numTrees=10, seed=123, maxDepth=8, maxBins=10)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DT(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    rf = DecisionTreeClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LR(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    lr = LogisticRegression(labelCol=\"fraud\", featuresCol=\"features\", maxIter=10)\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GB(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    lr = GBTClassifier(labelCol=\"fraud\", featuresCol=\"features\", maxIter=10)\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SVM(train, test, worker=1):\n",
    "    start = time()\n",
    "    \n",
    "    lsvc = LinearSVC(labelCol=\"fraud\", maxIter=10, regParam=0.1)\n",
    "    pipeline = Pipeline(stages=[lsvc])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.select('category').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "categories = sorted(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for category in categories:\n",
    "    temp = []\n",
    "    print(f'Category {category}:')\n",
    "    temp.append(category)\n",
    "    train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "    train = train.filter(train.category==category)\n",
    "    test = test.filter(test.category==category)\n",
    "    if len(train.groupBy('fraud').count().collect()) == 1:\n",
    "        temp = [category]\n",
    "        results.append(temp)\n",
    "        continue\n",
    "    train, test = smote(train, test)\n",
    "    auc_arr = []\n",
    "    acc_arr = []\n",
    "    duration_arr = []\n",
    "    auc_roc, acc, duration = evaluate_SVM(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_DT(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_RF(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_LR(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_GB(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    temp.extend(auc_arr)\n",
    "    temp.extend(acc_arr)\n",
    "    temp.extend(duration_arr)\n",
    "    results.append(temp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results,columns =['category', 'auc_roc_SVM', 'auc_roc_DT', 'auc_roc_RF', 'auc_roc_LR', 'auc_roc_GB', 'acc_SVM', 'acc_DT', 'acc_RF', 'acc_LR', 'acc_GB', 'time_SVM', 'time_DT', 'time_RF', 'time_LR', 'time_GB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxBins, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxDepth, [4, 6, 8])\\\n",
    "#     .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "#     .build()\n",
    "\n",
    "# # In this case the estimator is simply the linear regression.\n",
    "# # A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# tvs = TrainValidationSplit(estimator=rf,\n",
    "#                            estimatorParamMaps=paramGrid,\n",
    "#                            evaluator=roc_evaluator,\n",
    "#                            # 80% of the data will be used for training, 20% for validation.\n",
    "#                            trainRatio=0.8)\n",
    "\n",
    "# # Run TrainValidationSplit, and choose the best set of parameters.\n",
    "# model = tvs.fit(train)\n",
    "# predictions = model.transform(validation).select(\"features\", \"fraud\", \"prediction\")\n",
    "# print('AUC_ROC: {}'.format(roc_evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customers_df(df):\n",
    "    data_temp = {}\n",
    "    for row in df.collect():\n",
    "        temp = data_temp.get(row.customer, {})\n",
    "        temp_rows = temp.get('rows', [])\n",
    "        temp_rows.append(list(row.asDict().values()))\n",
    "        temp['rows'] = temp_rows\n",
    "        temp['count'] = temp.get('count', 0) + 1\n",
    "        data_temp[row.customer] = temp\n",
    "    for k,v in data_temp.items():\n",
    "        data_temp[k]['df'] = detector.spark.createDataFrame(data=v['rows'], schema=test.schema)\n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = create_customers_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customers_group(customers_dict, max_differ_count):\n",
    "    max_transaction_count = customers_dict[max(customers.keys(), key=(lambda k: customers[k]['count']))]['count']\n",
    "    min_transaction_count = customers_dict[min(customers.keys(), key=(lambda k: customers[k]['count']))]['count']\n",
    "    groups = {}\n",
    "    for i in range(min_transaction_count, max_transaction_count+1):\n",
    "        groups[i] = [item for k, item in customers_dict.items() if item['count']==i]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = create_customers_group(customers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "train, test = smote(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
