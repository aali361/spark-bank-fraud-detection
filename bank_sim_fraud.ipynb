{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "            getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "\n",
    "        \n",
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "    \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions: 8\n",
      "Created df with: 594643, 10\n"
     ]
    }
   ],
   "source": [
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/bank_sim.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def drop_columns(self, columns = ['zipcodeOri', 'zipMerchant']):\n",
    "        for col in columns:\n",
    "            self.df = self.df.drop('zipcodeOri')\n",
    "            self.df = self.df.drop('zipMerchant')\n",
    "    \n",
    "    def index_column(self, df, column):\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"Index\")\n",
    "        df = indexer.fit(df).transform(df)\n",
    "        df = df.withColumn(column, df[column+\"Index\"].cast(IntegerType()))\n",
    "        df = df.drop(column+\"Index\")\n",
    "        return df\n",
    "    \n",
    "    def columns_indexing(self, columns = ['merchant', 'category', 'customer', 'age', 'gender']):\n",
    "        for column in columns:\n",
    "            self.df = self.index_column(self.df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(detector.rep_data)\n",
    "preprocess.drop_columns()\n",
    "preprocess.columns_indexing()\n",
    "detector.rep_data = preprocess.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.merchant_fraud_probablites = None\n",
    "    \n",
    "    def merchant_fraud_probablity(self, merchant):\n",
    "        merchant_df = self.df.filter(self.df.merchant==merchant)\n",
    "        return merchant_df.filter(merchant_df.fraud==1).count()/merchant_df.count()\n",
    "    \n",
    "    def merchants_fruad_probablity(self):\n",
    "        merchants = self.df.toPandas()['merchant'].unique()\n",
    "        self.merchant_fraud_probablites = {}\n",
    "        for merchant in merchants:\n",
    "            self.merchant_fraud_probablites[merchant] = self.merchant_fraud_probablity(int(merchant))\n",
    "        return self.merchant_fraud_probablites\n",
    "    \n",
    "    @staticmethod\n",
    "    def merchant_probablity(merchant, x):\n",
    "        return probs[merchant]\n",
    "    \n",
    "    def extract_merchant_fraud_probablity(self, probs):\n",
    "        self.df = self.df.rdd.map(lambda x: x + (func1(x[\"merchant\"], probs),)).toDF(self.df.columns + [\"merchantProbablity\"])\n",
    "        \n",
    "    def customer_trans(self):\n",
    "        _customers_trans = {}\n",
    "        for row in self.df.collect():\n",
    "            customer_trans = _customers_trans.get(row.customer, None)\n",
    "            if customer_trans is None:\n",
    "                customer_trans = {}\n",
    "            if row.step in customer_trans:\n",
    "                customer_trans[row.step+1] = row.amount\n",
    "            else:\n",
    "                customer_trans[row.step] = row.amount\n",
    "            _customers_trans[row.customer] = customer_trans\n",
    "        customer_trans_broadcast = detector.spark.sparkContext.broadcast(_customers_trans)\n",
    "        return customer_trans_broadcast\n",
    "        \n",
    "    def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "        customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "        trans = [v for k,v in customer_trans.items() if k < step]\n",
    "    #     check this, must be 0 or -1\n",
    "        prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "        if int(prev_amount) == 0: return 0.0\n",
    "        variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "        return variance\n",
    "        \n",
    "    def extract_last_variance(self, customer_trans_broadcast):\n",
    "        self.df = self.df.rdd.map(lambda x: x + (last_variance(int(x[\"customer\"]), int(x['step']), int(x[\"amount\"]), customer_trans_broadcast),)).toDF(self.df.columns + [\"lastVariance\"])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(merchant, probs):\n",
    "    return probs[merchant]\n",
    "\n",
    "def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "    customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "    trans = [v for k,v in customer_trans.items() if k < step]\n",
    "    prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "    if int(prev_amount) == 0: return 0.0\n",
    "    variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "    return variance\n",
    "\n",
    "featureExtraction = FeatureExtraction(detector.rep_data)\n",
    "featureExtraction.extract_merchant_fraud_probablity(featureExtraction.merchants_fruad_probablity())\n",
    "customer_trans_broadcast = featureExtraction.customer_trans()\n",
    "df = featureExtraction.extract_last_variance(customer_trans_broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "df = df.withColumn('lastVariance', func.round(df[\"lastVariance\"], 4))\n",
    "df = df.withColumn('merchantProbablity', func.round(df[\"merchantProbablity\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('features')\n",
    "# inputCols = [\n",
    "#  'customer',\n",
    "#  'age',\n",
    "#  'gender',\n",
    "#  'merchant',\n",
    "#  'category',\n",
    "#  'amount',\n",
    "    \n",
    "#  'merchantProbablity',\n",
    "#  'lastVariance'\n",
    "# ]\n",
    "# assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "# df = assembler.transform(df)\n",
    "\n",
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "\n",
    "def vectorizerFunction(dataInput, TargetFieldName):\n",
    "    if(dataInput.select(TargetFieldName).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    columnNames = list(dataInput.columns)\n",
    "    columnNames.remove(TargetFieldName)\n",
    "    dataInput = dataInput.select((','.join(columnNames)+','+TargetFieldName).split(',')) \n",
    "    assembler=VectorAssembler(inputCols = columnNames, outputCol = 'features')\n",
    "    pos_vectorized = assembler.transform(dataInput)\n",
    "    vectorized = pos_vectorized.select('features',TargetFieldName).withColumn('label',pos_vectorized[TargetFieldName]).drop(TargetFieldName)\n",
    "    return vectorized\n",
    "\n",
    "def SmoteSampling(vectorized, k = 5, minorityClass = 1, majorityClass = 0, percentageOver = 200, percentageUnder = 100):\n",
    "    if(percentageUnder > 100|percentageUnder < 10):\n",
    "        raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "    if(percentageOver < 100):\n",
    "        raise ValueError(\"Percentage Over must be in at least 100\");\n",
    "    dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "    dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "    feature = dataInput_min.select('features').rdd.map(lambda x: x[0]).collect()\n",
    "    feature = np.asarray(feature)\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=k, algorithm='auto').fit(feature)\n",
    "    neighbours =  nbrs.kneighbors(feature)\n",
    "    gap = neighbours[0]\n",
    "    neighbours = neighbours[1]\n",
    "    pos_ListArray = dataInput_min.drop('label').rdd.map(lambda x : list(x)).collect()\n",
    "    min_Array = list(pos_ListArray)\n",
    "    newRows = []\n",
    "    nt = len(min_Array)\n",
    "    nexs = percentageOver/100\n",
    "    for i in range(nt):\n",
    "        for j in range(int(nexs)):\n",
    "            neigh = random.randint(1,k)\n",
    "            difs = min_Array[neigh][0] - min_Array[i][0]\n",
    "            newRec = (min_Array[i][0]+random.random()*difs)\n",
    "            newRows.insert(0,(newRec))\n",
    "    newData_rdd = detector.spark.sparkContext.parallelize(newRows)\n",
    "    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n",
    "    new_data = newData_rdd_new.toDF()\n",
    "    new_data_minor = dataInput_min.unionAll(new_data)\n",
    "    new_data_major = dataInput_maj.sample(False, (float(percentageUnder)/float(100)))\n",
    "    return new_data_major.unionAll(new_data_minor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(train, test):\n",
    "    train = vectorizerFunction(train, 'fraud')\n",
    "    test = vectorizerFunction(test, 'fraud')\n",
    "\n",
    "    # percentage under reduce major class. 100 mean all instances of major class\n",
    "    # percentage over increase minor class. 200 mean each sample of minor class must be doubled\n",
    "    train = SmoteSampling(train, k=2, minorityClass=1, majorityClass=0, percentageOver=200, percentageUnder=100)\n",
    "\n",
    "    train = train.withColumnRenamed('label','fraud')\n",
    "    test = test.withColumnRenamed('label','fraud')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RF(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol=\"fraud\", featuresCol=\"features\", numTrees=10, seed=123, maxDepth=8, maxBins=10)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "    print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DT(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    rf = DecisionTreeClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "    print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.select('category').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "categories = sorted(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for category in categories:\n",
    "    temp = []\n",
    "    print(f'Category {category}:')\n",
    "    temp.append(category)\n",
    "    train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "    train = train.filter(train.category==category)\n",
    "    test = test.filter(test.category==category)\n",
    "    if len(train.groupBy('fraud').count().collect()) == 1:\n",
    "        temp = [category, 0.0, 0.0, 0.0, 0.0]\n",
    "        results.append(temp)\n",
    "        continue\n",
    "    train, test = smote(train, test)\n",
    "    auc_roc, acc = evaluate_DT(train, test)\n",
    "    temp.extend([auc_roc, acc])\n",
    "    auc_roc, acc = evaluate_RF(train, test)\n",
    "    temp.extend([auc_roc, acc])\n",
    "    results.append(temp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results,columns =['category' ,'auc_roc_DT', 'acc_DT', 'auc_roc_RF', 'acc_RF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxBins, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxDepth, [4, 6, 8])\\\n",
    "#     .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "#     .build()\n",
    "\n",
    "# # In this case the estimator is simply the linear regression.\n",
    "# # A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# tvs = TrainValidationSplit(estimator=rf,\n",
    "#                            estimatorParamMaps=paramGrid,\n",
    "#                            evaluator=roc_evaluator,\n",
    "#                            # 80% of the data will be used for training, 20% for validation.\n",
    "#                            trainRatio=0.8)\n",
    "\n",
    "# # Run TrainValidationSplit, and choose the best set of parameters.\n",
    "# model = tvs.fit(train)\n",
    "# predictions = model.transform(validation).select(\"features\", \"fraud\", \"prediction\")\n",
    "# print('AUC_ROC: {}'.format(roc_evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customers_df(df):\n",
    "    data_temp = {}\n",
    "    for row in df.collect():\n",
    "        temp = data_temp.get(row.customer, {})\n",
    "        temp_rows = temp.get('rows', [])\n",
    "        temp_rows.append(list(row.asDict().values()))\n",
    "        temp['rows'] = temp_rows\n",
    "        temp['count'] = temp.get('count', 0) + 1\n",
    "        data_temp[row.customer] = temp\n",
    "    for k,v in data_temp.items():\n",
    "        data_temp[k]['df'] = detector.spark.createDataFrame(data=v['rows'], schema=test.schema)\n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = create_customers_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customers_group(customers_dict, max_differ_count):\n",
    "    max_transaction_count = customers_dict[max(customers.keys(), key=(lambda k: customers[k]['count']))]['count']\n",
    "    min_transaction_count = customers_dict[min(customers.keys(), key=(lambda k: customers[k]['count']))]['count']\n",
    "    groups = {}\n",
    "    for i in range(min_transaction_count, max_transaction_count+1):\n",
    "        groups[i] = [item for k, item in customers_dict.items() if item['count']==i]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = create_customers_group(customers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "train, test = smote(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparktorch import serialize_torch_obj, SparkTorch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.GRU(10, 20, 2)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pytorch object\n",
    "torch_obj = serialize_torch_obj(\n",
    "    model=network,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_model = SparkTorch(\n",
    "    inputCol='features',\n",
    "    labelCol='fraud',\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=50,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used in a pipeline and saved.\n",
    "Pipeline(stages=[spark_model]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
