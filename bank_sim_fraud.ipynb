{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "from time import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "            getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "\n",
    "        \n",
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "    \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions: 8\n",
      "Created df with: 594643, 10\n"
     ]
    }
   ],
   "source": [
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/bank_sim.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = detector.rep_data\n",
    "df = df.drop('zipcodeOri')\n",
    "df = df.drop('zipMerchant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_column(df, column):\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column+\"Index\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    df = df.withColumn(column, df[column+\"Index\"].cast(IntegerType()))\n",
    "    df = df.drop(column+\"Index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = index_column(df, 'merchant')\n",
    "df = index_column(df, 'category')\n",
    "df = index_column(df, 'customer')\n",
    "df = index_column(df, 'age')\n",
    "df = index_column(df, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merchant_fraud_probablity(merchant):\n",
    "    merchant_df = df.filter(df.merchant==merchant)\n",
    "    return merchant_df.filter(merchant_df.fraud==1).count()/merchant_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = df.toPandas()['merchant'].unique()\n",
    "merchants_fraud_probablity = {}\n",
    "for merchant in merchants:\n",
    "    merchants_fraud_probablity[merchant] = merchant_fraud_probablity(int(merchant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merchant_probablity(merchant):\n",
    "    return merchants_fraud_probablity[merchant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rdd.map(lambda x: x + (merchant_probablity(x[\"merchant\"]),)).toDF(df.columns + [\"merchantProbablity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_trans = {}\n",
    "for row in df.collect():\n",
    "    customer_trans = customers_trans.get(row.customer, None)\n",
    "    if customer_trans is None:\n",
    "        customer_trans = {}\n",
    "    if row.step in customer_trans:\n",
    "        customer_trans[row.step+1] = row.amount\n",
    "    else:\n",
    "        customer_trans[row.step] = row.amount\n",
    "    customers_trans[row.customer] = customer_trans\n",
    "customer_trans_broadcast = detector.spark.sparkContext.broadcast(customers_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "    customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "    trans = [v for k,v in customer_trans.items() if k < step]\n",
    "#     check this, must be 0 or -1\n",
    "    prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "    if int(prev_amount) == 0: return 0.0\n",
    "    variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('lastVariance')\n",
    "df = df.rdd.map(lambda x: x + (last_variance(int(x[\"customer\"]), int(x['step']), int(x[\"amount\"]), customer_trans_broadcast),)).toDF(df.columns + [\"lastVariance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "df = df.withColumn('lastVariance', func.round(df[\"lastVariance\"], 4))\n",
    "df = df.withColumn('merchantProbablity', func.round(df[\"merchantProbablity\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('features')\n",
    "# inputCols = [\n",
    "#  'customer',\n",
    "#  'age',\n",
    "#  'gender',\n",
    "#  'merchant',\n",
    "#  'category',\n",
    "#  'amount',\n",
    "    \n",
    "#  'merchantProbablity',\n",
    "#  'lastVariance'\n",
    "# ]\n",
    "# assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "# df = assembler.transform(df)\n",
    "\n",
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "\n",
    "def vectorizerFunction(dataInput, TargetFieldName):\n",
    "    if(dataInput.select(TargetFieldName).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    columnNames = list(dataInput.columns)\n",
    "    columnNames.remove(TargetFieldName)\n",
    "    dataInput = dataInput.select((','.join(columnNames)+','+TargetFieldName).split(','))\n",
    "    assembler=VectorAssembler(inputCols = columnNames, outputCol = 'features')\n",
    "    pos_vectorized = assembler.transform(dataInput)\n",
    "    vectorized = pos_vectorized.select('features',TargetFieldName).withColumn('label',pos_vectorized[TargetFieldName]).drop(TargetFieldName)\n",
    "    return vectorized\n",
    "\n",
    "def SmoteSampling(vectorized, k = 5, minorityClass = 1, majorityClass = 0, percentageOver = 200, percentageUnder = 100):\n",
    "    if(percentageUnder > 100|percentageUnder < 10):\n",
    "        raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "    if(percentageOver < 100):\n",
    "        raise ValueError(\"Percentage Over must be in at least 100\");\n",
    "    dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "    dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "    feature = dataInput_min.select('features').rdd.map(lambda x: x[0]).collect()\n",
    "    feature = np.asarray(feature)\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=k, algorithm='auto').fit(feature)\n",
    "    neighbours =  nbrs.kneighbors(feature)\n",
    "    gap = neighbours[0]\n",
    "    neighbours = neighbours[1]\n",
    "    pos_ListArray = dataInput_min.drop('label').rdd.map(lambda x : list(x)).collect()\n",
    "    min_Array = list(pos_ListArray)\n",
    "    newRows = []\n",
    "    nt = len(min_Array)\n",
    "    nexs = percentageOver/100\n",
    "    for i in range(nt):\n",
    "        for j in range(int(nexs)):\n",
    "            neigh = random.randint(1,k)\n",
    "            difs = min_Array[neigh][0] - min_Array[i][0]\n",
    "            newRec = (min_Array[i][0]+random.random()*difs)\n",
    "            newRows.insert(0,(newRec))\n",
    "    newData_rdd = detector.spark.sparkContext.parallelize(newRows)\n",
    "    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n",
    "    new_data = newData_rdd_new.toDF()\n",
    "    new_data_minor = dataInput_min.unionAll(new_data)\n",
    "    new_data_major = dataInput_maj.sample(False, (float(percentageUnder)/float(100)))\n",
    "    return new_data_major.unionAll(new_data_minor)\n",
    "\n",
    "train = vectorizerFunction(train, 'fraud')\n",
    "test = vectorizerFunction(test, 'fraud')\n",
    "\n",
    "# percentage under reduce major class. 100 mean all instances of major class\n",
    "# percentage over increase minor class. 200 mean each sample of minor class must be doubled\n",
    "train = SmoteSampling(train, k=2, minorityClass=1, majorityClass=0, percentageOver=200, percentageUnder=100)\n",
    "\n",
    "train = train.withColumnRenamed('label','fraud')\n",
    "test = test.withColumnRenamed('label','fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_ROC: 0.8700260094885565\n",
      "accuracy: 0.9942573033041681\n",
      "Elapsed time is: 51.91985845565796\n"
     ]
    }
   ],
   "source": [
    "# With 1 Worker\n",
    "start = time()\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"fraud\", featuresCol=\"features\", numTrees=10, seed=123, maxDepth=8, maxBins=10)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "\n",
    "roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "print('AUC_ROC: {}'.format(roc_evaluator.evaluate(predictions)))\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print('accuracy: {}'.format(accuracy_evaluator.evaluate(predictions)))\n",
    "\n",
    "print('Elapsed time is: {}'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxBins, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxDepth, [4, 6, 8])\\\n",
    "#     .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "#     .build()\n",
    "\n",
    "# # In this case the estimator is simply the linear regression.\n",
    "# # A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# tvs = TrainValidationSplit(estimator=rf,\n",
    "#                            estimatorParamMaps=paramGrid,\n",
    "#                            evaluator=roc_evaluator,\n",
    "#                            # 80% of the data will be used for training, 20% for validation.\n",
    "#                            trainRatio=0.8)\n",
    "\n",
    "# # Run TrainValidationSplit, and choose the best set of parameters.\n",
    "# model = tvs.fit(train)\n",
    "# predictions = model.transform(validation).select(\"features\", \"fraud\", \"prediction\")\n",
    "# print('AUC_ROC: {}'.format(roc_evaluator.evaluate(predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
