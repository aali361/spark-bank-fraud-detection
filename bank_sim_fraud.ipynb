{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from scipy import stats\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "            getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "\n",
    "        \n",
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "    \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"class\", metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions: 8\n",
      "Created df with: 594643, 10\n"
     ]
    }
   ],
   "source": [
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/bank_sim.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def drop_columns(self, columns = ['zipcodeOri', 'zipMerchant']):\n",
    "        for col in columns:\n",
    "            self.df = self.df.drop('zipcodeOri')\n",
    "            self.df = self.df.drop('zipMerchant')\n",
    "    \n",
    "    def index_column(self, df, column):\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"Index\")\n",
    "        df = indexer.fit(df).transform(df)\n",
    "        df = df.withColumn(column, df[column+\"Index\"].cast(IntegerType()))\n",
    "        df = df.drop(column+\"Index\")\n",
    "        return df\n",
    "    \n",
    "    def columns_indexing(self, columns = ['merchant', 'category', 'customer', 'age', 'gender']):\n",
    "        for column in columns:\n",
    "            self.df = self.index_column(self.df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(detector.rep_data)\n",
    "preprocess.drop_columns()\n",
    "preprocess.columns_indexing()\n",
    "detector.rep_data = preprocess.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.merchant_fraud_probablites = None\n",
    "    \n",
    "    def merchant_fraud_probablity(self, merchant):\n",
    "        merchant_df = self.df.filter(self.df.merchant==merchant)\n",
    "        return merchant_df.filter(merchant_df.fraud==1).count()/merchant_df.count()\n",
    "    \n",
    "    def merchants_fruad_probablity(self):\n",
    "        merchants = self.df.toPandas()['merchant'].unique()\n",
    "        self.merchant_fraud_probablites = {}\n",
    "        for merchant in merchants:\n",
    "            self.merchant_fraud_probablites[merchant] = self.merchant_fraud_probablity(int(merchant))\n",
    "        return self.merchant_fraud_probablites\n",
    "    \n",
    "    @staticmethod\n",
    "    def merchant_probablity(merchant, x):\n",
    "        return probs[merchant]\n",
    "    \n",
    "    def extract_merchant_fraud_probablity(self, probs):\n",
    "        self.df = self.df.rdd.map(lambda x: x + (func1(x[\"merchant\"], probs),)).toDF(self.df.columns + [\"merchantProbablity\"])\n",
    "        \n",
    "    def customer_trans(self):\n",
    "        _customers_trans = {}\n",
    "        for row in self.df.collect():\n",
    "            customer_trans = _customers_trans.get(row.customer, None)\n",
    "            if customer_trans is None:\n",
    "                customer_trans = {}\n",
    "            if row.step in customer_trans:\n",
    "                customer_trans[row.step+1] = row.amount\n",
    "            else:\n",
    "                customer_trans[row.step] = row.amount\n",
    "            _customers_trans[row.customer] = customer_trans\n",
    "        customer_trans_broadcast = detector.spark.sparkContext.broadcast(_customers_trans)\n",
    "        return customer_trans_broadcast\n",
    "        \n",
    "    def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "        customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "        trans = [v for k,v in customer_trans.items() if k < step]\n",
    "    #     check this, must be 0 or -1\n",
    "        prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "        if int(prev_amount) == 0: return 0.0\n",
    "        variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "        return variance\n",
    "        \n",
    "    def extract_last_variance(self, customer_trans_broadcast):\n",
    "        self.df = self.df.rdd.map(lambda x: x + (last_variance(int(x[\"customer\"]), int(x['step']), int(x[\"amount\"]), customer_trans_broadcast),)).toDF(self.df.columns + [\"lastVariance\"])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(merchant, probs):\n",
    "    return probs[merchant]\n",
    "\n",
    "def last_variance(customer, step, current_amount, customer_trans_broadcast):\n",
    "    customer_trans = customer_trans_broadcast.value.get(customer)\n",
    "    trans = [v for k,v in customer_trans.items() if k < step]\n",
    "    prev_amount = trans[-1] if len(trans) > 0 else 0\n",
    "    if int(prev_amount) == 0: return 0.0\n",
    "    variance = (int(current_amount) - int(prev_amount))/int(prev_amount)\n",
    "    return variance\n",
    "\n",
    "featureExtraction = FeatureExtraction(detector.rep_data)\n",
    "featureExtraction.extract_merchant_fraud_probablity(featureExtraction.merchants_fruad_probablity())\n",
    "customer_trans_broadcast = featureExtraction.customer_trans()\n",
    "df = featureExtraction.extract_last_variance(customer_trans_broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "df = df.withColumn('lastVariance', func.round(df[\"lastVariance\"], 4))\n",
    "df = df.withColumn('merchantProbablity', func.round(df[\"merchantProbablity\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('features')\n",
    "# inputCols = [\n",
    "#  'customer',\n",
    "#  'age',\n",
    "#  'gender',\n",
    "#  'merchant',\n",
    "#  'category',\n",
    "#  'amount',\n",
    "    \n",
    "#  'merchantProbablity',\n",
    "#  'lastVariance'\n",
    "# ]\n",
    "# assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "# df = assembler.transform(df)\n",
    "\n",
    "# train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "def vectorizerFunction(dataInput, TargetFieldName):\n",
    "    if(dataInput.select(TargetFieldName).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    columnNames = list(dataInput.columns)\n",
    "    columnNames.remove(TargetFieldName)\n",
    "    dataInput = dataInput.select((','.join(columnNames)+','+TargetFieldName).split(',')) \n",
    "    assembler=VectorAssembler(inputCols = columnNames, outputCol = 'features')\n",
    "    pos_vectorized = assembler.transform(dataInput)\n",
    "    vectorized = pos_vectorized.select('features',TargetFieldName).withColumn('label',pos_vectorized[TargetFieldName]).drop(TargetFieldName)\n",
    "    return vectorized\n",
    "\n",
    "def SmoteSampling(vectorized, k = 5, minorityClass = 1, majorityClass = 0, percentageOver = 200, percentageUnder = 100):\n",
    "    if(percentageUnder > 100|percentageUnder < 10):\n",
    "        raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "    if(percentageOver < 100):\n",
    "        raise ValueError(\"Percentage Over must be in at least 100\");\n",
    "    dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "    dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "    feature = dataInput_min.select('features').rdd.map(lambda x: x[0]).collect()\n",
    "    feature = np.asarray(feature)\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=k, algorithm='auto').fit(feature)\n",
    "    neighbours =  nbrs.kneighbors(feature)\n",
    "    gap = neighbours[0]\n",
    "    neighbours = neighbours[1]\n",
    "    pos_ListArray = dataInput_min.drop('label').rdd.map(lambda x : list(x)).collect()\n",
    "    min_Array = list(pos_ListArray)\n",
    "    newRows = []\n",
    "    nt = len(min_Array)\n",
    "    nexs = percentageOver/100\n",
    "    for i in range(nt):\n",
    "        for j in range(int(nexs)):\n",
    "            neigh = random.randint(1,k)\n",
    "            difs = min_Array[neigh][0] - min_Array[i][0]\n",
    "            newRec = (min_Array[i][0]+random.random()*difs)\n",
    "            newRows.insert(0,(newRec))\n",
    "    newData_rdd = detector.spark.sparkContext.parallelize(newRows)\n",
    "    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n",
    "    new_data = newData_rdd_new.toDF()\n",
    "    new_data_minor = dataInput_min.unionAll(new_data)\n",
    "    new_data_major = dataInput_maj.sample(False, (float(percentageUnder)/float(100)))\n",
    "    return new_data_major.unionAll(new_data_minor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(train, test, sampling=True):\n",
    "    train = vectorizerFunction(train, 'fraud')\n",
    "    test = vectorizerFunction(test, 'fraud')\n",
    "\n",
    "    # percentage under reduce major class. 100 mean all instances of major class\n",
    "    # percentage over increase minor class. 200 mean each sample of minor class must be doubled\n",
    "    if sampling:\n",
    "        train = SmoteSampling(train, k=2, minorityClass=1, majorityClass=0, percentageOver=200, percentageUnder=100)\n",
    "\n",
    "    train = train.withColumnRenamed('label','fraud')\n",
    "    test = test.withColumnRenamed('label','fraud')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count: 417107, validation count: 59473, test count: 118063\n"
     ]
    }
   ],
   "source": [
    "# must remove\n",
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "# train, test = smote(train, test, False)\n",
    "print('train count: {}, validation count: {}, test count: {}'.format(train.count(), validation.count(), test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customers_df(df):\n",
    "    x = df.select(['customer', 'gender', 'age']).dropDuplicates(['customer'])\n",
    "    y = df.groupby('customer').sum('amount')\n",
    "    y = y.join(x, on=['customer'], how='inner')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_customers(df):\n",
    "    customers = get_customer_df(train)\n",
    "    assemble=VectorAssembler(inputCols=['age','gender', 'sum(amount)'], outputCol='clustering_features')\n",
    "    assembled_data=assemble.transform(customers)\n",
    "    # assembled_data.show(2)\n",
    "    scale=StandardScaler(inputCol='clustering_features',outputCol='clustering_features_standardized')\n",
    "    data_scale=scale.fit(assembled_data)\n",
    "    data_scale_output=data_scale.transform(assembled_data)\n",
    "    # data_scale_output.show(2)\n",
    "    silhouette_score=[]\n",
    "    evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='clustering_features_standardized', \\\n",
    "                                    metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "    for i in range(2,10):\n",
    "        KMeans_algo=KMeans(featuresCol='clustering_features_standardized', k=i)\n",
    "        KMeans_fit=KMeans_algo.fit(data_scale_output)\n",
    "        output=KMeans_fit.transform(data_scale_output) \n",
    "        score=evaluator.evaluate(output)\n",
    "        silhouette_score.append(score)\n",
    "        print(\"Silhouette Score:\",score)\n",
    "\n",
    "    #Visualizing the silhouette scores in a plot\n",
    "    fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "    ax.plot(range(2,10),silhouette_score)\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = get_customers_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.4391226325095771\n",
      "Silhouette Score: 0.5666210123718302\n",
      "Silhouette Score: 0.46188269008006805\n",
      "Silhouette Score: 0.47638327610956854\n",
      "Silhouette Score: 0.645959393990168\n",
      "Silhouette Score: 0.6534655910430988\n",
      "Silhouette Score: 0.6432507465061351\n",
      "Silhouette Score: 0.63822393342299\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_customers(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = smote(train, test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panda = df.toPandas()\n",
    "y = np.array(df_panda['fraud'], dtype='float')\n",
    "X = df_panda.drop(['fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply using FFNN\n",
    "def simple_ffnn(train, test):\n",
    "    print('simple FFNN')\n",
    "    layers = [9, 5, 2]\n",
    "    start = time()\n",
    "    FNN = MultilayerPerceptronClassifier(labelCol=\"fraud\", featuresCol=\"features\", maxIter=100, layers=layers, blockSize=128, seed=123)\n",
    "    pipeline = Pipeline(stages=[FNN])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    print(time()-start)\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "#     predictions.show(5)\n",
    "    return auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to dense vector\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).toDF(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(train, test):\n",
    "    print('Ensemble')\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(labelCol=\"fraud\", featuresCol=\"features\", numTrees=10, seed=123, maxDepth=8, maxBins=10)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model_rf = pipeline.fit(train)\n",
    "    predictions_rf = model_rf.transform(train)\n",
    "    predictions_train_rf = predictions_rf.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_rf = model_rf.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[dt])\n",
    "    model_dt = pipeline.fit(train)\n",
    "    predictions_dt = model_dt.transform(train)\n",
    "    predictions_train_dt = predictions_dt.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_dt = model_dt.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(labelCol=\"fraud\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    model_lr = pipeline.fit(train)\n",
    "    predictions_lr = model_lr.transform(train)\n",
    "    predictions_train_lr = predictions_lr.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_lr = model_lr.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # Gradient Boosted\n",
    "    gb = GBTClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[gb])\n",
    "    model_gb = pipeline.fit(train)\n",
    "    predictions_gb = model_gb.transform(train)\n",
    "    predictions_train_gb = predictions_gb.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_gb = model_gb.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "    # SVM\n",
    "    svm = LinearSVC(labelCol=\"fraud\", maxIter=10, regParam=0.1)\n",
    "    pipeline = Pipeline(stages=[svm])\n",
    "    model_svm = pipeline.fit(train)\n",
    "    predictions_svm = model_svm.transform(train)\n",
    "    predictions_train_svm = predictions_svm.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    predictions_test_svm = model_svm.transform(test).select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "    \n",
    "    train_labels = train.select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "    test_labels = test.select('fraud').rdd.flatMap(lambda x:x).collect()\n",
    "    ffnn = []\n",
    "    for i in range(train.count()):\n",
    "        ffnn.append([predictions_train_rf[i], predictions_train_dt[i], predictions_train_lr[i], predictions_train_gb[i], predictions_train_svm[i], train_labels[i]])\n",
    "    models_predict = []\n",
    "    for i in range(test.count()):\n",
    "        models_predict.append([predictions_test_rf[i], predictions_test_dt[i], predictions_test_lr[i], predictions_test_gb[i], predictions_test_svm[i], test_labels[i]])\n",
    "        \n",
    "    train_df = detector.spark.createDataFrame(ffnn)\n",
    "    train_df = transData(train_df)\n",
    "    print(train_df.count())\n",
    "#     train_df.show()\n",
    "    \n",
    "    test_df = detector.spark.createDataFrame(models_predict)\n",
    "    test_df = transData(test_df)\n",
    "    print(test_df.count())\n",
    "#     test_df.show()\n",
    "    \n",
    "    layers = [5, 5, 4, 4, 3, 2]\n",
    "    # layers = [5, 2]\n",
    "    \n",
    "    FNN = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=100, layers=layers, blockSize=128, seed=123)\n",
    "    pipeline = Pipeline(stages=[FNN])\n",
    "    model = pipeline.fit(train_df)\n",
    "    predictions = model.transform(test_df)\n",
    "#     predictions.show(5)\n",
    "    \n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "    return auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(default=5):\n",
    "    auc_roc_arr_method1 = []\n",
    "    auc_roc_arr_method2 = []\n",
    "    for i in range(default):\n",
    "        skf = StratifiedKFold(n_splits=2)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            X_train.insert(1, 'fraud', y_train)\n",
    "            X_test.insert(1, 'fraud', y_test)\n",
    "            df_train = detector.spark.createDataFrame(X_train)\n",
    "            df_test = detector.spark.createDataFrame(X_test)\n",
    "            df_train, df_test = smote(df_train, df_test, False)\n",
    "            auc_roc_arr_method1.append(simple_ffnn(df_train, df_test))\n",
    "            auc_roc_arr_method2.append(ensemble(df_train, df_test))\n",
    "    return auc_roc_arr_method1, auc_roc_arr_method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_arr_method1, auc_roc_arr_method2 = repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pvalue(num_round, result_method1, result_method2):\n",
    "    score_diff = []\n",
    "    score_diff_2 = []\n",
    "    for i in range(num_round):\n",
    "        score_diff.append(auc_roc_arr_method1[i]-auc_roc_arr_method2[i])\n",
    "        score_diff_2.append((auc_roc_arr_method1[i]-auc_roc_arr_method2[i])**2)\n",
    "    d = sum(score_diff)\n",
    "    d2 = sum(score_diff_2)\n",
    "    numerator = d\n",
    "    denominator = np.sqrt((num_round*d2 - d**2)/ (num_round - 1))\n",
    "    t = numerator / denominator\n",
    "    pvalue = stats.t.sf(np.abs(t), num_round - 1)*2.\n",
    "    print(pvalue)\n",
    "\n",
    "    if pvalue <= 0.05:\n",
    "        print('Difference between mean performance is probably real')\n",
    "    else:\n",
    "        print('Algorithms probably have the same performance')\n",
    "    return pvalue, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pvalue(10, auc_roc_arr_method1, auc_roc_arr_method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RF(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol=\"fraud\", featuresCol=\"features\", numTrees=10, seed=123, maxDepth=8, maxBins=10)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DT(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    rf = DecisionTreeClassifier(labelCol=\"fraud\", featuresCol=\"features\", seed=123)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LR(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    lr = LogisticRegression(labelCol=\"fraud\", featuresCol=\"features\", maxIter=10)\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GB(train, test, worker=1):\n",
    "    start = time()\n",
    "\n",
    "    lr = GBTClassifier(labelCol=\"fraud\", featuresCol=\"features\", maxIter=10)\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SVM(train, test, worker=1):\n",
    "    start = time()\n",
    "    \n",
    "    lsvc = LinearSVC(labelCol=\"fraud\", maxIter=10, regParam=0.1)\n",
    "    pipeline = Pipeline(stages=[lsvc])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    predictions = predictions.withColumn(\"fraud\", predictions[\"fraud\"].cast(DoubleType()))\n",
    "#     print(f'Elapsed time is: {time()-start}, with {worker} worker')\n",
    "\n",
    "    roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = roc_evaluator.evaluate(predictions)\n",
    "    print(f'AUC_ROC: {auc_roc}')\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f'Accuracy: {acc}')\n",
    "    \n",
    "    return auc_roc, acc, time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.select('category').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "categories = sorted(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for category in categories:\n",
    "    temp = []\n",
    "    print(f'Category {category}:')\n",
    "    temp.append(category)\n",
    "    train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "    train = train.filter(train.category==category)\n",
    "    test = test.filter(test.category==category)\n",
    "    if len(train.groupBy('fraud').count().collect()) == 1:\n",
    "        temp = [category]\n",
    "        results.append(temp)\n",
    "        continue\n",
    "    train, test = smote(train, test)\n",
    "    auc_arr = []\n",
    "    acc_arr = []\n",
    "    duration_arr = []\n",
    "    auc_roc, acc, duration = evaluate_SVM(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_DT(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_RF(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_LR(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    auc_roc, acc, duration = evaluate_GB(train, test)\n",
    "    auc_arr.append(auc_roc)\n",
    "    acc_arr.append(acc)\n",
    "    duration_arr.append(duration)\n",
    "    temp.extend(auc_arr)\n",
    "    temp.extend(acc_arr)\n",
    "    temp.extend(duration_arr)\n",
    "    results.append(temp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results,columns =['category', 'auc_roc_SVM', 'auc_roc_DT', 'auc_roc_RF', 'auc_roc_LR', 'auc_roc_GB', 'acc_SVM', 'acc_DT', 'acc_RF', 'acc_LR', 'acc_GB', 'time_SVM', 'time_DT', 'time_RF', 'time_LR', 'time_GB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxBins, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxDepth, [4, 6, 8])\\\n",
    "#     .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "#     .build()\n",
    "\n",
    "# # In this case the estimator is simply the linear regression.\n",
    "# # A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# tvs = TrainValidationSplit(estimator=rf,\n",
    "#                            estimatorParamMaps=paramGrid,\n",
    "#                            evaluator=roc_evaluator,\n",
    "#                            # 80% of the data will be used for training, 20% for validation.\n",
    "#                            trainRatio=0.8)\n",
    "\n",
    "# # Run TrainValidationSplit, and choose the best set of parameters.\n",
    "# model = tvs.fit(train)\n",
    "# predictions = model.transform(validation).select(\"features\", \"fraud\", \"prediction\")\n",
    "# print('AUC_ROC: {}'.format(roc_evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customers_df(df):\n",
    "    data_temp = {}\n",
    "    for row in df.collect():\n",
    "        temp = data_temp.get(row.customer, {})\n",
    "        temp_rows = temp.get('rows', [])\n",
    "        temp_rows.append(list(row.asDict().values()))\n",
    "        temp['rows'] = temp_rows\n",
    "        temp['count'] = temp.get('count', 0) + 1\n",
    "        data_temp[row.customer] = temp\n",
    "    for k,v in data_temp.items():\n",
    "        data_temp[k]['df'] = detector.spark.createDataFrame(data=v['rows'], schema=test.schema)\n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = create_customers_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customers_group(customers_dict, max_differ_count):\n",
    "    max_transaction_count = customers_dict[max(customers.keys(), key=(lambda k: customers[k]['count']))]['count']\n",
    "    min_transaction_count = customers_dict[min(customers.keys(), key=(lambda k: customers[k]['count']))]['count']\n",
    "    groups = {}\n",
    "    for i in range(min_transaction_count, max_transaction_count+1):\n",
    "        groups[i] = [item for k, item in customers_dict.items() if item['count']==i]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = create_customers_group(customers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = df.randomSplit([0.7, 0.1, 0.2], seed=123)\n",
    "train, test = smote(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
