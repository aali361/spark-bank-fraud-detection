{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import *\n",
    "from time import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, LSTM, GRU\n",
    "from tensorflow.python.keras import optimizers, regularizers\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetection():\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.data = None\n",
    "        self.rep_data = None # repeated data\n",
    "        \n",
    "    def create_spark_context(self, ram, rpt=False, ret=False):\n",
    "        self.spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Fraud Detector\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"{}g\".format(ram)).\\\n",
    "            getOrCreate()\n",
    "        if rpt: print(self.spark.sparkContext.getConf().getAll())\n",
    "        if ret: return self.spark\n",
    "    \n",
    "    def read_file(self, path, rpt=False, ret=False):\n",
    "        self.data = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        if rpt: print('number of partitions: {}'.format(self.data.rdd.getNumPartitions()))\n",
    "        if ret: return self.data\n",
    "    \n",
    "    def data_duplicator(self, number, rpt=False, ret=False):\n",
    "        self.rep_data = self.data\n",
    "        for i in range(number-1):\n",
    "            self.rep_data = self.data.union(self.rep_data)\n",
    "        if rpt: print(\"Created df with: {}, {}\".format(self.rep_data .count(), len(self.rep_data .columns)))\n",
    "        if ret: return self.rep_data\n",
    "\n",
    "        \n",
    "class Preprocess():\n",
    "    def __init__(self, data):\n",
    "        self.spark = None\n",
    "        self.sub_sample = None\n",
    "        self.data = data\n",
    "        \n",
    "    def scale_column(self, feature):\n",
    "        self.data = self.data.withColumn(feature, self.data[feature].cast(IntegerType()))\n",
    "        assembler = VectorAssembler().setInputCols([feature]).setOutputCol('f'+feature)\n",
    "        self.data = assembler.transform(self.data)\n",
    "        self.data = self.data.drop(feature)\n",
    "        scaler = RobustScaler(inputCol=\"f\"+feature, outputCol=feature,\n",
    "                          withScaling=True, withCentering=False,\n",
    "                          lower=0.25, upper=0.75)\n",
    "        scalerModel = scaler.fit(self.data)\n",
    "        self.data = scalerModel.transform(self.data)\n",
    "        self.data = self.data.drop('f'+feature)\n",
    "        unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "        self.data = self.data.withColumn(feature, unlist(feature))\n",
    "        return self.data\n",
    "    \n",
    "    def robust_scale(self, scale_columns):\n",
    "        for column in scale_columns:\n",
    "            self.data = self.scale_column(column)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_iqr_bound(self, feature, q1, q3, k, rpt=False):\n",
    "        bound = self.sub_sample.filter(self.data.Class==1).approxQuantile(feature, [q1, q3], 0)\n",
    "        if rpt: print(f'Feature: {feature}, Lower bound: {bound[0]}, Upper bound: {bound[1]}')\n",
    "        iqr = bound[1] - bound[0]\n",
    "        if rpt: print(f'Feature: {feature}, IQR: {iqr}')\n",
    "        bound[0] = bound[0] - (iqr * k)\n",
    "        bound[1] = bound[1] + (iqr * k)\n",
    "        if rpt: print(f'Feature: {feature}, Cut-off Lower bound: {bound[0]}, Cut-off Upper bound: {bound[1]}')\n",
    "        return bound\n",
    "    \n",
    "    def outlier_removal(self, features, q1=0.25, q3=0.75, k=1.5, rpt=False):\n",
    "        frauds = self.data.filter(self.data.Class==1)\n",
    "        self.sub_sample = frauds.union(self.data.filter(self.data.Class==0).limit(492))\n",
    "        for feature in features:\n",
    "            before_removal_count = self.sub_sample.count()\n",
    "            bound = self.calculate_iqr_bound(feature, q1, q3, k, rpt=rpt)\n",
    "            self.sub_sample = self.sub_sample.filter((col(feature) >= bound[0]) & (col(feature) <= bound[1]))\n",
    "            after_removal_count = self.sub_sample.count()\n",
    "            if rpt: print(f'before removal count: {before_removal_count}, after removal count: {after_removal_count}')\n",
    "    \n",
    "    def assemble_features(self):\n",
    "        assembler = VectorAssembler(inputCols=['V{}'.format(i) for i in range(1,29)], outputCol='features')\n",
    "        self.data = assembler.transform(self.data)\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self, label=\"class\", prediction=\"prediction\"):\n",
    "        self.label = label\n",
    "        self.prediction = prediction\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=self.label, predictionCol=self.prediction, metricName=\"accuracy\")\n",
    "        print('accuracy: {}'.format(accuracy_evaluator.evaluate(data)))\n",
    "        \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=self.label, predictionCol=self.prediction, metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data)))\n",
    "    \n",
    "    def recall(self, data):\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=self.label, predictionCol=self.prediction, metricName=\"recallByLabel\")\n",
    "        print('recall: {}'.format(recall_evaluator.evaluate(data))) \n",
    "    \n",
    "    def auc_roc(self, data):\n",
    "        roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=self.prediction, labelCol=self.label, metricName=\"areaUnderROC\")\n",
    "        auc_roc = roc_evaluator.evaluate(data)\n",
    "        print(f'auc_roc: {auc_roc}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keras_Predictor():\n",
    "    def __init__(self):\n",
    "        self.model = None \n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(28,), activity_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        model.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        model.add(Dense(2))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        # Set and Serialize Optimizer\n",
    "        optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "        opt_conf = optimizers.serialize(optimizer_conf)\n",
    "        \n",
    "        # Initialize SparkML Estimator and Get Settings\n",
    "        estimator = ElephasEstimator()\n",
    "        estimator.setFeaturesCol(\"features\")\n",
    "        estimator.setLabelCol(\"Class\")\n",
    "        estimator.set_keras_model_config(model.to_yaml())\n",
    "        estimator.set_categorical_labels(True)\n",
    "        estimator.set_nb_classes(2)\n",
    "        estimator.set_num_workers(1)\n",
    "        estimator.set_epochs(25) \n",
    "        estimator.set_batch_size(64)\n",
    "        estimator.set_verbosity(1)\n",
    "        estimator.set_validation_split(0.10)\n",
    "        estimator.set_optimizer_config(opt_conf)\n",
    "        estimator.set_mode(\"synchronous\")\n",
    "        estimator.set_loss(\"binary_crossentropy\")\n",
    "        estimator.set_metrics(['acc'])\n",
    "\n",
    "        # Create Deep Learning Pipeline\n",
    "        self.dl_pipeline = Pipeline(stages=[estimator])\n",
    "        \n",
    "    def train(self, data):\n",
    "        dl_pipeline = self.dl_pipeline\n",
    "        start = time()\n",
    "        self.model = dl_pipeline.fit(data)\n",
    "        print('Elapsed time is: {}'.format(time()-start))\n",
    "    \n",
    "    def pred(self, data):\n",
    "        return self.model.transform(data).select('Class', \"prediction\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions: 8\n",
      "Created df with: 284807, 31\n",
      "Elapsed time is: 4.193568229675293\n"
     ]
    }
   ],
   "source": [
    "ram=16\n",
    "duplicate=1\n",
    "splitation=[0.7, 0.1, 0.2]\n",
    "detector = FraudDetection()\n",
    "detector.create_spark_context(ram=ram)\n",
    "detector.read_file(\"/opt/workspace/creditcard.csv\", True)\n",
    "detector.data_duplicator(duplicate, True)\n",
    "start = time()\n",
    "preprocessor = Preprocess(detector.rep_data)\n",
    "preprocessor.robust_scale(['Time', 'Amount'])\n",
    "preprocessor.outlier_removal(['V14', 'V12', 'V10'], rpt=False)\n",
    "detector.data = preprocessor.assemble_features()\n",
    "train, validation, test = detector.data.randomSplit(splitation)\n",
    "print('Elapsed time is: {}'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fd3175bf5d79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeras_Predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-3da540011b11>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lecun_uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         model.set_weights([np.random.rand(*w.shape) * 0.2 - 0.1 for w in model.get_weights()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, time_major, unroll, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m     self.state_spec = [\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_dropout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_recurrent_dropout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropoutRNNCellMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m   2683\u001b[0m         \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   2686\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, time_major, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "k = Keras_Predictor()\n",
    "k.train(train)\n",
    "predictions = k.pred(test)\n",
    "\n",
    "e = Evaluator(label=\"Class\", prediction=\"prediction\")\n",
    "e.accuracy(predictions)\n",
    "e.auc_roc(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 1 Worker\n",
    "\n",
    "start = time()\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", numTrees=10)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Class\", \"features\").show(5)\n",
    "predictions = predictions.withColumn(\"Class\", predictions[\"Class\"].cast(DoubleType()))\n",
    "\n",
    "roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"Class\", labelCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "print('ROC: {}'.format(roc_evaluator.evaluate(predictions)))\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"prediction\", predictionCol=\"Class\", metricName=\"accuracy\")\n",
    "print('accuracy: {}'.format(accuracy_evaluator.evaluate(predictions)))\n",
    "print('Elapsed time is: {}'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
